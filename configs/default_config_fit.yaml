# pytorch_lightning==1.9.4
seed_everything: true
ckpt_path: null # resuming training
trainer:
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      name: null
      project: WSI
      log_model: true
      entity: gipmed
      mode: online
  callbacks:
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      monitor: val/slide_auc
      filename: epoch={epoch}-val_auc={val/slide_auc:.3f}
      save_last: true
      save_top_k: 3
      mode: max
      auto_insert_metric_name: false
  # - class_path: pytorch_lightning.callbacks.EarlyStopping
  #   init_args:
  #     monitor: val/slide_auc
  #     min_delta: 0.0
  #     patience: 20
  #     mode: max
  stratagy: ddp_find_unused_parameters_false
  devices: auto
  check_val_every_n_epoch: 5
  fast_dev_run: false
  max_epochs: 650
  limit_train_batches: null # set to 0. to not perform training
  limit_val_batches: null # set to 0. to not perform validation
  accelerator: gpu
  num_sanity_val_steps: 2
  profiler: null
  auto_lr_find: false
  auto_scale_batch_size: false
model:
  model: resnet50
  lr: 2e-3
  lr_scheduler: true
  weight_decay: 2e-5
  num_classes: 2
  ckpt_path: null # for transfer learning
  imagenet_pretrained: false
  finetune: false
  criterion: crossentropy
  log_params: false
data:
  datasets_folds: {'CAT':[2,3,4,5]}
  target: er_status
  patches_per_slide_train: 10
  patches_per_slide_eval: 10
  min_tiles_eval: 100
  img_size: 256
  batch_size: 256
  num_workers: 8
  normalization: cat
  autoaug: wsi_ron
  transforms: null
  openslide: false
# override default optimizer and lr scheduler
# optimizer:
#   class_path: torch.optim.SGD
#   init_args:
#     lr: 0.1
#     momentum: 0.9
#     weight_decay: 1e-4
# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.StepLR
#   init_args:
#     step_size: 30
#     gamma: 0.1
