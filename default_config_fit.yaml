# pytorch_lightning==1.9.4
seed_everything: true
ckpt_path: null # resuming training, testing, feature extraction
trainer:
  logger:
    class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      name: null
      project: WSI
      log_model: true
      entity: gipmed
      mode: online
  callbacks:
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      monitor: val/slide_auc
      filename: epoch={epoch}-val_auc={val/slide_auc:.3f}
      save_last: true
      save_top_k: 3
      mode: max
      auto_insert_metric_name: false
  # - class_path: pytorch_lightning.callbacks.EarlyStopping
  #   init_args:
  #     monitor: val/slide_auc
  #     min_delta: 0.0
  #     patience: 20
  #     mode: max
  devices: auto
  gpus: null
  check_val_every_n_epoch: 1
  fast_dev_run: false
  max_epochs: 100
  limit_train_batches: null # set to 0. to not perform training
  limit_val_batches: null # set to 0. to not perform validation
  accelerator: gpu
  num_sanity_val_steps: 2
  profiler: null
  auto_lr_find: false
  auto_scale_batch_size: false
model:
  model: resnet50
  lr: 0.001
  num_classes: 2
  ckpt_path: null # for transfer learning
  imagenet_pretrained: false
  finetune: false
  criterion: crossentropy
  log_params: false
data:
  dataset: CAT
  target: ER
  val_fold: 1
  eval_on_train: false
  patches_per_slide_train: 10
  patches_per_slide_eval: 10
  img_size: 256
  batch_size: 128
  num_workers: 8
  normalization: cat
  autoaug: false
  transforms: null
  openslide: false
# override default optimizer and lr scheduler
# optimizer:
#   class_path: torch.optim.SGD
#   init_args:
#     lr: 0.1
#     momentum: 0.9
#     weight_decay: 1e-4
# lr_scheduler:
#   class_path: torch.optim.lr_scheduler.StepLR
#   init_args:
#     step_size: 30
#     gamma: 0.1
